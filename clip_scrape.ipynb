{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NBA possession data updater for 2025 season...\n",
      "\n",
      "Processing ATL...\n",
      "2025\n",
      "Updating ATL data from 2025-02-11 to 2025-02-24\n",
      "Fetched 231 possessions for ATL from 2025-02-11 to 2025-02-17\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "22400773\n",
      "Fetched 196 possessions for ATL from 2025-02-18 to 2025-02-24\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n",
      "22400790\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 281\u001b[0m\n\u001b[1;32m    279\u001b[0m season \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2025\u001b[39m  \u001b[38;5;66;03m# Current season\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting NBA possession data updater for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m season...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 281\u001b[0m \u001b[43mupdate_all_teams\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseason\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 275\u001b[0m, in \u001b[0;36mupdate_all_teams\u001b[0;34m(season, base_dir)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m team \u001b[38;5;129;01min\u001b[39;00m teams:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mteam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 275\u001b[0m     \u001b[43mupdate_team_possessions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseason\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 234\u001b[0m, in \u001b[0;36mupdate_team_possessions\u001b[0;34m(team, season, base_dir)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_possessions:\n\u001b[1;32m    233\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(new_possessions)\n\u001b[0;32m--> 234\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m \u001b[43mnew_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteam_games\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGAMEID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mleft\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# If file exists, append new data; otherwise create new file\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:10093\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10074\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m  10075\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m  10076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  10089\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m  10090\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m  10091\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m> 10093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  10094\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10095\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10096\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10097\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10098\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10099\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10102\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10103\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m  10107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/merge.py:707\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    699\u001b[0m (\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m    703\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1340\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1336\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1337\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1338\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1339\u001b[0m     ):\n\u001b[0;32m-> 1340\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on object and int64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import sys\n",
    "games_url ='https://raw.githubusercontent.com/gabriel1200/shot_data/refs/heads/master/game_dates.csv'\n",
    "games = pd.read_csv(games_url)\n",
    "def get_date_ranges(start_date, end_date):\n",
    "    \"\"\"Generate 7-day date ranges between start and end dates.\"\"\"\n",
    "    start = datetime.datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    \n",
    "    date_ranges = []\n",
    "    current = start\n",
    "    \n",
    "    while current < end:\n",
    "        range_end = min(current + timedelta(days=6), end)\n",
    "        date_ranges.append((\n",
    "            current.strftime('%Y-%m-%d'),\n",
    "            range_end.strftime('%Y-%m-%d')\n",
    "        ))\n",
    "        current = range_end + timedelta(days=1)\n",
    "    \n",
    "    return date_ranges\n",
    "def determine_season(date_str):\n",
    "    \"\"\"Determine the season based on a date string.\"\"\"\n",
    "    year = int(date_str[:4])\n",
    "    month = int(date_str[5:7])\n",
    "    \n",
    "    if month >= 9:  # New season starts around October\n",
    "        return f\"{year}-{str(year+1)[-2:]}\"\n",
    "    else:\n",
    "        return f\"{year-1}-{str(year)[-2:]}\"\n",
    "def fetch_possessions(team, start_date, end_date):\n",
    "    \"\"\"Fetch both offensive and defensive possessions for a team in the given date range.\"\"\"\n",
    "    team_dict = get_team_dict()\n",
    "    season = determine_season(start_date)\n",
    "    url = \"https://api.pbpstats.com/get-possessions/nba\"\n",
    "    \n",
    "    all_possessions = []\n",
    "    \n",
    "    # Fetch offensive possessions\n",
    "    params = {\n",
    "        \"league\": 'nba',\n",
    "        \"TeamId\": team_dict[team],\n",
    "        \"Season\": season,\n",
    "        \"SeasonType\": \"All\",\n",
    "        \"OffDef\": \"Offense\",\n",
    "        \"StartType\": \"All\",\n",
    "        \"FromDate\": start_date,\n",
    "        \"ToDate\": end_date,\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()  # Raise exception for HTTP errors\n",
    "        response_json = response.json()\n",
    "        offensive_possessions = response_json.get(\"possessions\", [])\n",
    "        \n",
    "        # Add team info to each possession\n",
    "        for possession in offensive_possessions:\n",
    "            possession['Team'] = team\n",
    "            possession['IsOffense'] = True\n",
    "        \n",
    "        all_possessions.extend(offensive_possessions)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching offensive possessions for {team}: {e}\")\n",
    "    \n",
    "    # Fetch defensive possessions\n",
    "    params['OffDef'] = \"Defense\"\n",
    "    time.sleep(2)\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status()\n",
    "        response_json = response.json()\n",
    "        defensive_possessions = response_json.get(\"possessions\", [])\n",
    "        \n",
    "        # Add team info to each possession\n",
    "        for possession in defensive_possessions:\n",
    "            possession['Team'] = team\n",
    "            possession['IsOffense'] = False\n",
    "        \n",
    "        all_possessions.extend(defensive_possessions)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching defensive possessions for {team}: {e}\")\n",
    "    \n",
    "    print(f\"Fetched {len(all_possessions)} possessions for {team} from {start_date} to {end_date}\")\n",
    "    return all_possessions\n",
    "def get_team_dict():\n",
    "    \"\"\"Returns a dictionary mapping team abbreviations to team IDs.\"\"\"\n",
    "    return {\n",
    "        'ATL': '1610612737', 'BKN': '1610612751', 'BOS': '1610612738', 'CHA': '1610612766',\n",
    "        'CHI': '1610612741', 'CLE': '1610612739', 'DAL': '1610612742', 'DEN': '1610612743',\n",
    "        'DET': '1610612765', 'GSW': '1610612744', 'HOU': '1610612745', 'IND': '1610612754',\n",
    "        'LAC': '1610612746', 'LAL': '1610612747', 'MEM': '1610612763', 'MIA': '1610612748',\n",
    "        'MIL': '1610612749', 'MIN': '1610612750', 'NOP': '1610612740', 'NYK': '1610612752',\n",
    "        'OKC': '1610612760', 'ORL': '1610612753', 'PHI': '1610612755', 'PHX': '1610612756',\n",
    "        'POR': '1610612757', 'SAC': '1610612758', 'SAS': '1610612759', 'TOR': '1610612761',\n",
    "        'UTA': '1610612762', 'WAS': '1610612764'\n",
    "    }\n",
    "\n",
    "def get_latest_date(file_path):\n",
    "    \"\"\"Get the latest date from an existing CSV file.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        if 'GAMEDATE' in df.columns:\n",
    "            return df['GAMEDATE'].max()\n",
    "        return None\n",
    "    except (FileNotFoundError, pd.errors.EmptyDataError):\n",
    "        return None\n",
    "\n",
    "def convert_new_to_old_format(possession_data, team_id):\n",
    "    \"\"\"Convert new possession data format to match the old CSV structure.\"\"\"\n",
    "    converted_data = []\n",
    "    \n",
    "    for possession in possession_data:\n",
    "        # Extract game teams from the GameId\n",
    "        game_id = possession.get('GameId', '')\n",
    "        game_id = game_id[2:]\n",
    "        print(game_id)\n",
    "      \n",
    "        # Convert events to string\n",
    "        events = ','.join(possession.get('Events', [])) if isinstance(possession.get('Events'), list) else str(possession.get('Events', ''))\n",
    "        \n",
    "        # Get video data\n",
    "        video_data = possession.get('VideoUrls', [])\n",
    "        \n",
    "        if isinstance(video_data, list) and video_data:\n",
    "            # Create a row for each video description/url pair\n",
    "            for video_item in video_data:\n",
    "                row = {\n",
    "                    'ENDTIME': possession.get('EndTime', ''),\n",
    "                    'EVENTS': events,\n",
    "                    'FG2A': possession.get('FG2A', 0),\n",
    "                    'FG2M': possession.get('FG2M', 0),\n",
    "                    'FG3A': possession.get('FG3A', 0),\n",
    "                    'FG3M': possession.get('FG3M', 0),\n",
    "                    'GAMEDATE': possession.get('GameDate', ''),\n",
    "                    'GAMEID': game_id,\n",
    "                    'NONSHOOTINGFOULSTHATRESULTEDINFTS': possession.get('NonShootingFoulsThatResultedInFts', 0),\n",
    "                    'OFFENSIVEREBOUNDS': possession.get('OffensiveRebounds', 0),\n",
    "                    'OPPONENT': possession.get('Opponent', ''),\n",
    "                    'PERIOD': possession.get('Period', ''),\n",
    "                    'SHOOTINGFOULSDRAWN': possession.get('ShootingFoulsDrawn', 0),\n",
    "                    'STARTSCOREDIFFERENTIAL': possession.get('StartScoreDifferential', 0),\n",
    "                    'STARTTIME': possession.get('StartTime', ''),\n",
    "                    'STARTTYPE': possession.get('StartType', ''),\n",
    "                    'TURNOVERS': possession.get('Turnovers', 0),\n",
    "                    'DESCRIPTION': video_item.get('description', ''),\n",
    "                    'URL': video_item.get('url', np.nan) if video_item.get('url') else np.nan,\n",
    "                    'team': possession.get('Team', ''),\n",
    "                    'TEAM_ID': team_id\n",
    "                }\n",
    "                converted_data.append(row)\n",
    "        else:\n",
    "            # If no video data, create a single row with empty description and URL\n",
    "            row = {\n",
    "                'ENDTIME': possession.get('EndTime', ''),\n",
    "                'EVENTS': events,\n",
    "                'FG2A': possession.get('FG2A', 0),\n",
    "                'FG2M': possession.get('FG2M', 0),\n",
    "                'FG3A': possession.get('FG3A', 0),\n",
    "                'FG3M': possession.get('FG3M', 0),\n",
    "                'GAMEDATE': possession.get('GameDate', ''),\n",
    "                'GAMEID': game_id,\n",
    "                'NONSHOOTINGFOULSTHATRESULTEDINFTS': possession.get('NonShootingFoulsThatResultedInFts', 0),\n",
    "                'OFFENSIVEREBOUNDS': possession.get('OffensiveRebounds', 0),\n",
    "                'OPPONENT': possession.get('Opponent', ''),\n",
    "                'PERIOD': possession.get('Period', ''),\n",
    "                'SHOOTINGFOULSDRAWN': possession.get('ShootingFoulsDrawn', 0),\n",
    "                'STARTSCOREDIFFERENTIAL': possession.get('StartScoreDifferential', 0),\n",
    "                'STARTTIME': possession.get('StartTime', ''),\n",
    "                'STARTTYPE': possession.get('StartType', ''),\n",
    "                'TURNOVERS': possession.get('Turnovers', 0),\n",
    "                'DESCRIPTION': '',\n",
    "                'URL': np.nan,\n",
    "                'team': possession.get('Team', ''),\n",
    "                'TEAM_ID': team_id\n",
    "            }\n",
    "            converted_data.append(row)\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "def update_team_possessions(team, season, base_dir='nba_possessions_data'):\n",
    "    print(season)\n",
    "    season_str = str(season-1)+'-'+str(season)[-2:]\n",
    "    team_games = games[games['team']==team]\n",
    "    team_games = team_games[team_games['season']==season_str]\n",
    "    #team_games['GAME_ID'] = '00'+team_games['GAME_ID'].astype(str)\n",
    "    #print(team_games.head())\n",
    "    team_games = team_games[['GAME_ID','VTM','HTM']]\n",
    "    team_games.columns = ['GAMEID','VTM','HTM']\n",
    "    team_games['GAMEID'] = team_games['GAMEID'].astype(int)\n",
    "    team_games.drop_duplicates(subset='GAMEID',inplace=True)\n",
    "    #print(team_games.head())\n",
    "    \"\"\"Update possession data for a specific team from their last recorded date.\"\"\"\n",
    "    team_dict = get_team_dict()\n",
    "    team_id = team_dict[team]\n",
    "    \n",
    "    # Construct file path and create directories if needed\n",
    "    season_dir = os.path.join(base_dir, str(season))\n",
    "    os.makedirs(season_dir, exist_ok=True)\n",
    "    file_path = os.path.join(season_dir, f\"{season}_{team}_possessions.csv\")\n",
    "    \n",
    "    # Get the latest date from existing file\n",
    "    start_date = get_latest_date(file_path)\n",
    "    if start_date is None:\n",
    "        start_date = f\"{season-1}-10-22\"  # Season start date if no file exists\n",
    "    else:\n",
    "        # Add one day to the latest date to avoid duplicates\n",
    "        start_date = (pd.to_datetime(start_date) + pd.Timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Set end date to current date\n",
    "    end_date = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    \n",
    "    print(f\"Updating {team} data from {start_date} to {end_date}\")\n",
    "    \n",
    "    # Fetch new possessions\n",
    "    date_ranges = get_date_ranges(start_date, end_date)\n",
    "    new_possessions = []\n",
    "    \n",
    "    for start, end in date_ranges:\n",
    "        possessions = fetch_possessions(team, start, end)\n",
    "        if possessions:\n",
    "            converted_possessions = convert_new_to_old_format(possessions, team_id)\n",
    "            new_possessions.extend(converted_possessions)\n",
    "        time.sleep(2)  # API rate limiting\n",
    "    \n",
    "    if new_possessions:\n",
    "        new_df = pd.DataFrame(new_possessions)\n",
    "        new_df = new_df.merge(team_games, on='GAMEID', how='left')\n",
    "\n",
    "        \n",
    "         \n",
    "        # If file exists, append new data; otherwise create new file\n",
    "        if os.path.exists(file_path):\n",
    "            existing_df = pd.read_csv(file_path)\n",
    "            #print(new_df.columns)\n",
    "            #print(existing_df.columns)\n",
    "            updated_df = pd.concat([existing_df, new_df], ignore_index=True)\n",
    "           \n",
    "            # Remove duplicates based on all columns except index\n",
    "            for col in updated_df.columns:\n",
    "\n",
    "            \n",
    "                updated_df[col] = updated_df[col].astype(existing_df[col].dtype)\n",
    "                \n",
    " \n",
    "        \n",
    "          \n",
    "         \n",
    "                    \n",
    "            updated_df.drop_duplicates(inplace=True)\n",
    "            \n",
    "            # Sort by date and time\n",
    "            updated_df = updated_df.sort_values(['GAMEDATE', 'PERIOD', 'STARTTIME'])\n",
    "        else:\n",
    "            updated_df = new_df\n",
    "\n",
    "        \n",
    "        updated_df.to_csv(file_path, index=False)\n",
    "        print(f\"Added {len(new_possessions)} new possessions to {file_path}\")\n",
    "    else:\n",
    "        print(f\"No new possessions found for {team}\")\n",
    "\n",
    "def update_all_teams(season=2025, base_dir='nba_possessions_data'):\n",
    "    \"\"\"Update possession data for all teams.\"\"\"\n",
    "    teams = list(get_team_dict().keys())\n",
    "    \n",
    "    for team in teams:\n",
    "        print(f\"\\nProcessing {team}...\")\n",
    "        update_team_possessions(team, season, base_dir)\n",
    "        time.sleep(2)  # Delay between teams\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    season = 2025  # Current season\n",
    "    print(f\"Starting NBA possession data updater for {season} season...\")\n",
    "    update_all_teams(season)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
